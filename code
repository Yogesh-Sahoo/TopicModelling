import pandas as pd
import re, pickle, os
import datetime
import nltk
from nltk.util import ngrams
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from nltk.corpus import stopwords, wordnet
from collections import Counter
from nltk.stem import WordNetLemmatizer
from gensim import corpora, models
from gensim.corpora import MmCorpus
from gensim.models.coherencemodel import CoherenceModel
import pyLDAvis.gensim


tweets_df = pd.read_csv('Twitter_Data_Postcovid (3).csv').drop("slno", axis="columns").drop("date", axis="columns"). \
    drop("username", axis="columns").drop("location", axis="columns").drop("sentiment", axis="columns")

additional_stop_words = ["big", "hurt", "saving", "review", "leave", "home", "without", "get", "loop", "see", "try", "number",
                         "assistance", "exotic", "skin", "year", "old","month", "ago","every", "someone", "see", "look",
                         "customer", "service", "care","connect","team","anything", "else","something", "assist", "would","like",
                         "answer", "phone", "dispute", "charge", "owner", "make", "kind", "word", "burger", "king", "let", 
                         "know", "even", "though", "insurance", "thanks", "thank",
                         "amex", "american", "express", "americanexpress", "mastercard", "thank you", "visa", "credit",
                         "card", "american express", "askamex", "billyjoel", "@billyjoel", "billy", 
                         "to", "hi", "ha", 'tt', 'ts', 'is', 'tnk', 'tnks', 'for', 'day', 'you', 'we', 've', 'now',
                         'wa', 'time', 'good',
                         'great', 'will', 'need', 'tral', 'awesome', 'deal', 'awsome', 'asome', 'on', 'hold', 'central',
                         'please',
                         'ia', 'really', 'ok', 'use', 'back', 'call', 'wt', 'help', 'people', 'ask', 'late fee',
                         'cancel', 'take',
                         'guy', 'called', 'bad', 'poor', 'money', 'back', 'amp', 'sorry', 'hear', 'email', 'got',
                         "fuck",
                         "trying to", "trying", "to", "fucking"]

NUM_GRAMS = 2
# ----------------------
# LDA model parameters
# ----------------------
# Number of topics
NUM_TOPICS = 15
# Number of training passes
NUM_PASSES = 50
# Document-Topic Density. The lower alpha is, the more likely that
# a document may contain mixture of just a few of the topics.
# Default is 1.0/NUM_TOPICS
ALPHA = 0.001
# Word-Topic Density. The lower eta is, the more likely that
# a topic may contain a mixture of just a few of the words
# Default is 1.0/NUM_TOPICS
ETA = 'auto'


# -----------


def get_wordnet_pos(word):
    """
    Map POS tag to first character lemmatize() accepts
    """
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)


def text_cleanup(text):
    """
    Text pre-processing
        return tokenized list of cleaned words
    """
    # Convert to lowercase
    text_clean = text.lower()
    # Remove non-alphabet
    text_clean = re.sub(r'[^a-zA-Z]|(\w+:\/\/\S+)', ' ', text_clean).split()
    # Remove short words (length < 3)
    text_clean = [w for w in text_clean if len(w) > 2]
    # Lemmatize text with the appropriate POS tag
    lemmatizer = WordNetLemmatizer()
    text_clean = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in text_clean]
    # Filter out stop words in English
    stops = set(stopwords.words('english')).union(additional_stop_words)
    text_clean = [w for w in text_clean if w not in stops]

    return text_clean


def wordcloud(word_count_df):
    """
    Create word cloud image
    """
    # Convert DataFrame to Map so that word cloud can be generated from freq
    word_count_dict = {}
    for w, f in word_count_df.values:
        word_count_dict[w] = f
    # Generate word cloud
    wordcloud = WordCloud(max_words=300, width=1400, height=900,
                          random_state=12, contour_width=3,
                          contour_color='firebrick')
    wordcloud.generate_from_frequencies(word_count_dict)
    plt.figure(figsize=(10, 10), facecolor='k')
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    # Save the word cloud image
    wordcloud.to_file("new_wordcloud_post.png")
    print('Word cloud saved\n')
    plt.close('all')

    return wordcloud


def get_word_count(tweets_text, num_gram):
    """
    Get common word counts
    """
    n_grams = list(ngrams(tweets_text, num_gram))
    common_words = Counter(n_grams).most_common()
    word_count = pd.DataFrame(data=common_words,
                              columns=['word', 'frequency'])
    # Convert list to string
    word_count['word'] = word_count['word'].apply(' '.join)
    # Plot word count graph
    word_count.head(20).sort_values('frequency').plot.barh(
        x='word', y='frequency', title='Word Frequency', figsize=(19, 10))
    plt.savefig("common_word_frequency_post.png")
    print('Word count saved\n')
    plt.close('all')

    return word_count


def word_grams(words, min=1, max=2):
    """
    Build ngrams word list
    """
    word_list = []
    for n in range(min, max):
        for ngram in ngrams(words, n):
            word_list.append(' '.join(str(i) for i in ngram))
    return word_list

def read_data_from_pickle(infile):
    with open (infile, 'rb') as fp:
        return pickle.load(fp)
 
def save_data_to_pickle(outfile, all_tweets):
    with open(outfile, 'wb') as fp:
        pickle.dump(all_tweets, fp)
   
def save_print_to_file(outfile, msg):
    with open(outfile, 'w') as fp:
        print(msg, file=fp)  
    


cleaned_tweets_df = tweets_df.copy(deep=True)
# parsing tweets
cleaned_tweets_df['token'] = [text_cleanup(x) for x in tweets_df['tweet']]
# Save cleaned tweets to file
save_data_to_pickle("tweets_cleaned_df_post.csv", cleaned_tweets_df)
print('Cleaned tweets saved\n')


def train_lda_model(token_tweets):
    print('Start LDA model training ...\n')
    # Build dictionary
    tweets_dict = corpora.Dictionary(token_tweets)
    # Remove words that occur less than 10 documents,
    # or more than 50% of the doc
    tweets_dict.filter_extremes(no_below=10, no_above=0.5)
    # Transform doc to a vectorized form by computing frequency of each word
    bow_corpus = [tweets_dict.doc2bow(doc) for doc in token_tweets]
    # Save corpus and dictionary to file
    MmCorpus.serialize("clean_tweets_corpus_post.mm", bow_corpus)
    tweets_dict.save('clean_tweets_post.dict')

    # Create tf-idf model and then apply transformation to the entire corpus
    tfidf = models.TfidfModel(bow_corpus)
    tfidf_corpus = tfidf[bow_corpus]

    # Train LDA model
    lda_model = models.ldamodel.LdaModel(corpus=tfidf_corpus,
                                         num_topics=NUM_TOPICS,
                                         id2word=tweets_dict,
                                         passes=NUM_PASSES,
                                         alpha=ALPHA,
                                         eta=ETA,
                                         random_state=49)
    # Save LDA model to file
    lda_model.save('tweets_lda_post.model')
    print('LDA model saved\n')

    # Save all generated topics to a file
    msg = ''
    for idx, topic in lda_model.print_topics(-1):
        msg += 'Topic: {} \nWords: {}\n'.format(idx, topic)
    save_print_to_file('tweets_lda_topics_post.csv', msg)

    # Evaluate LDA model performance
    eval_lda(lda_model, tfidf_corpus, tweets_dict, token_tweets)
    # Visualize topics
    vis_topics(lda_model, tfidf_corpus, tweets_dict)

    return lda_model


def eval_lda(lda_model, corpus, dict, token_text):
    # Compute Perplexity: a measure of how good the model is. lower the better.
    print('\nPerplexity: ', lda_model.log_perplexity(corpus))
    # Compute Coherence Score
    coherence_model_lda = CoherenceModel(model=lda_model, texts=token_text,
                                         dictionary=dict, coherence='c_v')
    print('\nCoherence: ', coherence_model_lda.get_coherence())


def vis_topics(lda_model, corpus, dict):
    """
    Plot generated topics on an interactive graph
    """
    lda_data = pyLDAvis.gensim.prepare(lda_model, corpus, dict, mds='mmds',R=12)
    pyLDAvis.display(lda_data)
    pyLDAvis.save_html(lda_data, 'lda_post.html')
    print('Topic visual saved\n')


if __name__ == '__main__':
    # Convert series to list for word count
    tweets_text = [word for one_tweet in cleaned_tweets_df['token'] for word in one_tweet]
    # Get common ngrams word count
    word_count_df = get_word_count(tweets_text, num_gram=NUM_GRAMS)
    # Generate word cloud
    tweets_wordcloud = wordcloud(word_count_df)
    # Generate ngram tokens
    cleaned_tweets_df['ngram_token'] = [word_grams(x, NUM_GRAMS, NUM_GRAMS+1) for
                     x in cleaned_tweets_df['token']]
    # Train LDA model and visualize generated topics
    lda_model = train_lda_model(cleaned_tweets_df['ngram_token'])
    print('DONE!')
